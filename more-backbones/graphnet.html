

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLoCa-GraphNet &mdash; LLoCa  documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=9edc463e" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="LLoCa-ParticleNet" href="particlenet.html" />
    <link rel="prev" title="LLoCa-ParticleTransformer" href="particletransformer.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            LLoCa
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../lloca-vs-lgatr.html">LLoCa vs L-GATr</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Make any network Lorentz-equivariant</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="transformer.html">LLoCa-Transformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="particletransformer.html">LLoCa-ParticleTransformer</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">LLoCa-GraphNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="particlenet.html">LLoCa-ParticleNet</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../numerics.html">Tips and tricks for stable LLoCa training</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">LLoCa</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Make any network Lorentz-equivariant</a></li>
      <li class="breadcrumb-item active">LLoCa-GraphNet</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/more-backbones/graphnet.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="lloca-graphnet">
<h1>LLoCa-GraphNet<a class="headerlink" href="#lloca-graphnet" title="Link to this heading">ÔÉÅ</a></h1>
<p>We now implement tensorial message-passing for a simple graph network based on edge convolutions, similar to <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EdgeConv.html">https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.EdgeConv.html</a>.</p>
<p>Tensorial message-passing is implemented conveniently by the <a class="reference internal" href="../generated/lloca.backbone.lloca_message_passing.LLoCaMessagePassing.html#lloca.backbone.lloca_message_passing.LLoCaMessagePassing" title="lloca.backbone.lloca_message_passing.LLoCaMessagePassing"><code class="xref py py-class docutils literal notranslate"><span class="pre">LLoCaMessagePassing</span></code></a> class
which extends the standard <code class="code docutils literal notranslate"><span class="pre">torch_geometric.nn.MessagePassing</span></code>
and implements tensorial message-passing using <code class="code docutils literal notranslate"><span class="pre">pre_propagate_hook</span></code> and <code class="code docutils literal notranslate"><span class="pre">pre_message_hook</span></code>.</p>
<div class="highlight-diff notranslate"><div class="highlight"><pre><span></span>import torch
from torch import nn
from torch.utils.checkpoint import checkpoint

<span class="gd">-from torch_geometric.nn import MessagePassing</span>
<span class="gi">+from ..reps.tensorreps import TensorReps</span>
<span class="gi">+from .lloca_message_passing import LLoCaMessagePassing</span>
from .mlp import MLP

<span class="gd">-class EdgeConv(MessagePassing):</span>
<span class="gi">+class EdgeConv(LLoCaMessagePassing):</span>
<span class="w"> </span>   def __init__(
<span class="w"> </span>       self,
<span class="w"> </span>       -features,
<span class="w"> </span>       +reps,
<span class="w"> </span>       num_layers_mlp1,
<span class="w"> </span>       num_layers_mlp2,
<span class="w"> </span>       aggr=&quot;add&quot;,
<span class="w"> </span>       num_edge_attr=0,
<span class="w"> </span>       dropout_prob=None,
<span class="w"> </span>   ):
<span class="w"> </span>       &quot;&quot;&quot;Simple edge convolution layer.

<span class="w"> </span>       Parameters
<span class="w"> </span>       ----------
<span class="gd">-       features : int</span>
<span class="gd">-           Number of latent features.</span>
<span class="gi">+       reps : TensorReps</span>
<span class="gi">+           Tensor representation used during message passing.</span>
<span class="w"> </span>       num_layers_mlp1 : int
<span class="w"> </span>           Number of hidden layers in the first MLP.
<span class="w"> </span>       num_layers_mlp2 : int
<span class="w"> </span>           Number of hidden layers in the second MLP.
<span class="w"> </span>           If 0, no second MLP is used.
<span class="w"> </span>       aggr : str
<span class="w"> </span>           Aggregation method. One of &quot;add&quot;, &quot;mean&quot;, or &quot;max&quot;.
<span class="w"> </span>       num_edge_attr : int
<span class="w"> </span>           Number of edge attributes.
<span class="w"> </span>       dropout_prob : float
<span class="w"> </span>           Dropout probability in the MLPs.
<span class="w"> </span>       &quot;&quot;&quot;
<span class="gd">-       super().__init__(aggr=aggr)</span>
<span class="gi">+       super().__init__(aggr=aggr, params_dict={&quot;x&quot;: {&quot;type&quot;: &quot;local&quot;, &quot;rep&quot;: reps}})</span>
<span class="gi">+       features = reps.dim</span>
<span class="w"> </span>       self.mlp1 = MLP(
<span class="w"> </span>           in_shape=[features * 2 + num_edge_attr],
<span class="w"> </span>           out_shape=[features],
<span class="w"> </span>           hidden_layers=num_layers_mlp1,
<span class="w"> </span>           hidden_channels=reps.dim,
<span class="w"> </span>           dropout_prob=dropout_prob,
<span class="w"> </span>       )
<span class="w"> </span>       self.mlp2 = (
<span class="w"> </span>           MLP(
<span class="w"> </span>               in_shape=[features],
<span class="w"> </span>               out_shape=[features],
<span class="w"> </span>               hidden_layers=num_layers_mlp2,
<span class="w"> </span>               hidden_channels=reps.dim,
<span class="w"> </span>               dropout_prob=dropout_prob,
<span class="w"> </span>           )
<span class="w"> </span>           if num_layers_mlp2 &gt; 0
<span class="w"> </span>           else nn.Identity()
<span class="w"> </span>       )

<span class="gd">-   def forward(self, x, edge_index, batch=None, edge_attr=None):</span>
<span class="gi">+   def forward(self, x, frames, edge_index, batch=None, edge_attr=None):</span>
<span class="w"> </span>       &quot;&quot;&quot;Forward pass.

<span class="w"> </span>       Parameters
<span class="w"> </span>       ----------
<span class="w"> </span>       x : Tensor
<span class="w"> </span>           Input data with shape (num_items, reps.dim)
<span class="gi">+       frames : Frames</span>
<span class="gi">+           Local frames used for message passing</span>
<span class="w"> </span>       edge_index : Tensor
<span class="w"> </span>           Edge index tensor with shape (2, num_edges)
<span class="w"> </span>       batch : Tensor
<span class="w"> </span>           Batch tensorwith shape (num_items,)

<span class="w"> </span>       Returns
<span class="w"> </span>       -------
<span class="w"> </span>       x_aggr : Tensor
<span class="w"> </span>           Outputs with shape (num_items, reps.dim)
<span class="w"> </span>       &quot;&quot;&quot;
<span class="w"> </span>       frames = (frames, frames)

<span class="w"> </span>       x_aggr = self.propagate(
<span class="w"> </span>           edge_index,
<span class="w"> </span>           x=x,
<span class="gi">+           frames=frames,</span>
<span class="w"> </span>           edge_attr=edge_attr,
<span class="w"> </span>           batch=batch,
<span class="w"> </span>       )
<span class="w"> </span>       x_aggr = self.mlp2(x_aggr)
<span class="w"> </span>       return x_aggr

<span class="gd">-   def message(self, x_i, x_j, edge_attr=None):</span>
<span class="gi">+   def message(self, x_i, x_j, frames_i, frames_j, edge_attr=None):</span>
<span class="w"> </span>       x = x_j
<span class="w"> </span>       x = torch.cat((x, x_i), dim=-1)
<span class="w"> </span>       if edge_attr is not None:
<span class="w"> </span>           x = torch.cat((x, edge_attr), dim=-1)
<span class="w"> </span>       x = self.mlp1(x)
<span class="w"> </span>       return x


class GraphNet(nn.Module):
<span class="w"> </span>   &quot;&quot;&quot;Baseline LLoCa-GNN.

<span class="w"> </span>   Simple message-passing graph neural network, consisting of EdgeConv blocks.

<span class="w"> </span>   Parameters
<span class="w"> </span>   ----------
<span class="w"> </span>   in_channels : int
<span class="w"> </span>       Number of input channels.
<span class="gd">-    hidden_features: int</span>
<span class="gd">-        Number of latent features.</span>
<span class="gi">+   hidden_reps : str</span>
<span class="gi">+       Tensor representation used in the hidden layers.</span>
<span class="w"> </span>   out_channels : int
<span class="w"> </span>       Number of output channels.
<span class="w"> </span>   num_blocks : int
<span class="w"> </span>       Number of EdgeConv blocks.
<span class="w"> </span>   *args
<span class="w"> </span>   checkpoint_blocks : bool
<span class="w"> </span>       Whether to use gradient checkpointing in the EdgeConv blocks.
<span class="w"> </span>   **kwargs
<span class="w"> </span>   &quot;&quot;&quot;

<span class="w"> </span>   def __init__(
<span class="w"> </span>       self,
<span class="w"> </span>       in_channels: int,
<span class="w"> </span>       hidden_reps: str,
<span class="w"> </span>       out_channels: int,
<span class="w"> </span>       num_blocks: int,
<span class="w"> </span>       *args,
<span class="w"> </span>       checkpoint_blocks=False,
<span class="w"> </span>       **kwargs,
<span class="w"> </span>   ):
<span class="w"> </span>       super().__init__()
<span class="gi">+       hidden_reps = TensorReps(hidden_reps)</span>
<span class="w"> </span>       self.checkpoint_blocks = checkpoint_blocks

<span class="gd">-       self.linear_in = nn.Linear(in_channels, hidden_features)</span>
<span class="gd">-       self.linear_out = nn.Linear(hidden_features, out_channels)</span>
<span class="gi">+       self.linear_out = nn.Linear(hidden_reps.dim, out_channels)</span>
<span class="gi">+       self.linear_in = nn.Linear(in_channels, hidden_reps.dim)</span>
<span class="w"> </span>       self.blocks = nn.ModuleList(
<span class="w"> </span>           [
<span class="w"> </span>               EdgeConv(
<span class="gd">-                   hidden_features,</span>
<span class="gi">+                   hidden_reps,</span>
<span class="w"> </span>                   *args,
<span class="w"> </span>                   **kwargs,
<span class="w"> </span>               )
<span class="w"> </span>               for _ in range(num_blocks)
<span class="w"> </span>           ]
<span class="w"> </span>       )

<span class="gd">-   def forward(self, inputs, edge_index, batch=None, edge_attr=None):</span>
<span class="gi">+   def forward(self, inputs, frames, edge_index, batch=None, edge_attr=None):</span>
<span class="w"> </span>       &quot;&quot;&quot;Forward pass.

<span class="w"> </span>       Parameters
<span class="w"> </span>       ----------
<span class="w"> </span>       inputs : Tensor
<span class="w"> </span>           Input data with shape (num_items, in_channels)
<span class="gi">+       frames : Frames</span>
<span class="gi">+           Local frames used for message passing</span>
<span class="w"> </span>       edge_index : Tensor
<span class="w"> </span>           Edge index tensor with shape (2, num_edges)
<span class="w"> </span>       batch : Tensor
<span class="w"> </span>           Batch tensorwith shape (num_items,)
<span class="w"> </span>           If None, assumes fully connected graph along the num_items direction.
<span class="w"> </span>       edge_attr : Tensor
<span class="w"> </span>           Edge attribute tensor with shape (num_edges, num_edge_attr)

<span class="w"> </span>       Returns
<span class="w"> </span>       -------
<span class="w"> </span>       outputs : Tensor
<span class="w"> </span>           Outputs with shape (num_items, out_channels)
<span class="w"> </span>       &quot;&quot;&quot;
<span class="w"> </span>       x = self.linear_in(inputs)
<span class="w"> </span>       for block in self.blocks:
<span class="w"> </span>           if self.checkpoint_blocks:
<span class="w"> </span>               x = checkpoint(
<span class="w"> </span>                   block,
<span class="w"> </span>                   x=x,
<span class="gi">+                   frames=frames,</span>
<span class="w"> </span>                   edge_index=edge_index,
<span class="w"> </span>                   batch=batch,
<span class="w"> </span>                   edge_attr=edge_attr,
<span class="w"> </span>                   use_reentrant=False,
<span class="w"> </span>               )
<span class="w"> </span>           else:
<span class="w"> </span>               x = block(
<span class="w"> </span>                   x=x,
<span class="gi">+                   frames=frames,</span>
<span class="w"> </span>                   edge_index=edge_index,
<span class="w"> </span>                   batch=batch,
<span class="w"> </span>                   edge_attr=edge_attr,
<span class="w"> </span>               )
<span class="w"> </span>       outputs = self.linear_out(x)
<span class="w"> </span>       return outputs
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="particletransformer.html" class="btn btn-neutral float-left" title="LLoCa-ParticleTransformer" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="particlenet.html" class="btn btn-neutral float-right" title="LLoCa-ParticleNet" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Jonas Spinner.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>